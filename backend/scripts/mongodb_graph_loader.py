#!/usr/bin/env python3
"""
MongoDB JSON-LD to Graph Loader with Vector Search Support

This script loads JSON-LD data from MongoDB (generated by mongodb_ttl_generator.py) 
and saves them as graph structures in MongoDB Atlas for GraphRAG-style querying 
and analysis, with support for vector embeddings.

Requirements:
- pymongo
- rdflib
- python-dotenv (optional, for environment variables)
- sentence-transformers (for generating embeddings)

Usage:
    python mongodb_graph_loader.py --database parliamentary_graph [--skip-embeddings] [--limit N] [--video-id VIDEO_ID]
"""

import sys
import os
import json
import argparse
from typing import Dict, List, Any, Optional
from urllib.parse import urlparse
import hashlib
from datetime import datetime, timezone
import re

try:
    from pymongo import MongoClient, ASCENDING, TEXT
    from pymongo.errors import ConnectionFailure, DuplicateKeyError
    from rdflib import Graph, URIRef, Literal, BNode
    from rdflib.namespace import RDF, RDFS, OWL, FOAF, XSD
    from dotenv import load_dotenv
except ImportError as e:
    print(f"Missing required package: {e}")
    print("Please install required packages:")
    print("pip install pymongo rdflib python-dotenv")
    sys.exit(1)

# Optional: Import sentence transformers for embeddings
try:
    from sentence_transformers import SentenceTransformer
    EMBEDDINGS_AVAILABLE = True
except ImportError:
    EMBEDDINGS_AVAILABLE = False
    print("⚠️  sentence-transformers not available. Install with: pip install sentence-transformers")

# Load environment variables
load_dotenv()

class MongoDBGraphLoader:
    def __init__(self, connection_string: str = None, database_name: str = "parliamentary_graph", 
                 use_embeddings: bool = True):
        """
        Initialize the MongoDB graph loader.
        
        Args:
            connection_string: MongoDB Atlas connection string. If None, will try to get from environment.
            database_name: Name of the MongoDB database to use
            use_embeddings: Whether to generate vector embeddings for text content
        """
        if connection_string is None:
            connection_string = os.getenv('MONGODB_CONNECTION_STRING')
            
        if not connection_string:
            raise ValueError(
                "MongoDB connection string is required. Set MONGODB_CONNECTION_STRING environment variable "
                "or pass connection_string parameter."
            )
        
        try:
            self.client = MongoClient(connection_string)
            # Test connection
            self.client.admin.command('ping')
            print("✅ Successfully connected to MongoDB Atlas")
        except ConnectionFailure as e:
            raise ConnectionFailure(f"Failed to connect to MongoDB: {e}")
        
        self.db = self.client[database_name]
        self.use_embeddings = use_embeddings and EMBEDDINGS_AVAILABLE
        
        # Source collection (videos with JSON-LD data)
        self.videos_source = self.db.videos
        
        # Initialize embedding model if available
        if self.use_embeddings:
            try:
                print("🔄 Loading embedding model...")
                self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
                print("✅ Embedding model loaded successfully")
            except Exception as e:
                print(f"⚠️  Failed to load embedding model: {e}")
                self.use_embeddings = False
        
        self.setup_collections()
    
    def setup_collections(self):
        """Set up MongoDB collections with appropriate indexes."""
        
        # Nodes collection for entities
        self.nodes = self.db.nodes
        try:
            self.nodes.create_index([("uri", ASCENDING)], unique=True)
        except:
            pass  # Index already exists
        try:
            self.nodes.create_index([("type", ASCENDING)])
        except:
            pass
        try:
            self.nodes.create_index([("label", TEXT), ("searchable_text", TEXT)])
        except:
            pass
        try:
            self.nodes.create_index([("source_video", ASCENDING)])
        except:
            pass
        
        # Edges collection for relationships
        self.edges = self.db.edges
        try:
            self.edges.create_index([("subject", ASCENDING)])
        except:
            pass
        try:
            self.edges.create_index([("predicate", ASCENDING)])
        except:
            pass
        try:
            self.edges.create_index([("object", ASCENDING)])
        except:
            pass
        try:
            self.edges.create_index([("source_video", ASCENDING)])
        except:
            pass
        try:
            self.edges.create_index([("subject", ASCENDING), ("predicate", ASCENDING), ("object", ASCENDING)], unique=True)
        except:
            pass
        
        # Statements collection for reified statements (provenance)
        self.statements = self.db.statements
        
        # Drop and recreate the problematic index
        try:
            self.statements.drop_index("statement_id_1")
        except:
            pass  # Index doesn't exist
        
        try:
            self.statements.create_index([("global_statement_id", ASCENDING)], unique=True)
        except:
            pass
        try:
            self.statements.create_index([("statement_id", ASCENDING)])
        except:
            pass
        try:
            self.statements.create_index([("subject", ASCENDING)])
        except:
            pass
        try:
            self.statements.create_index([("predicate", ASCENDING)])
        except:
            pass
        try:
            self.statements.create_index([("object", ASCENDING)])
        except:
            pass
        try:
            self.statements.create_index([("source_video", ASCENDING)])
        except:
            pass
        try:
            self.statements.create_index([("start_offset", ASCENDING)])
        except:
            pass
        try:
            self.statements.create_index([("end_offset", ASCENDING)])
        except:
            pass
        
        print("✅ Collections and indexes set up successfully")
        
        if self.use_embeddings:
            print("ℹ️  Vector search indexes need to be created manually in Atlas:")
            print("   1. Go to your Atlas cluster")
            print("   2. Navigate to Search > Create Search Index")
            print("   3. Choose 'Vector Search' and use the nodes collection")
            print("   4. Use this configuration:")
            print("""   {
     "fields": [
       {
         "type": "vector",
         "path": "embedding",
         "numDimensions": 384,
         "similarity": "cosine"
       }
     ]
   }""")
    
    def get_videos_with_jsonld(self, limit: Optional[int] = None, video_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        Get videos with JSON-LD data that haven't been processed into graph format yet,
        or that were processed with an older version of the loader.
        
        Args:
            limit: Maximum number of videos to process
            video_id: Process only a specific video ID
            
        Returns:
            List of video documents with JSON-LD data
        """
        current_version = "mongodb_graph_loader_v2.0"
        
        # Build query to find videos that need (re)processing
        query = {
            "json_ld": {"$exists": True, "$ne": None},
            "$or": [
                {"graph_processed": {"$ne": True}},  # Never processed
                {"graph_processing_version": {"$ne": current_version}},  # Wrong version
                {"graph_processing_version": {"$exists": False}}  # No version recorded
            ]
        }
        
        if video_id:
            query["video_id"] = video_id
        
        # Only fetch necessary fields
        projection = {
            "video_id": 1,
            "Video_title": 1,
            "VideoURL": 1,
            "json_ld": 1,
            "rdf_triple_count": 1,
            "graph_processed": 1,
            "graph_processing_version": 1,
            "_id": 1
        }
        
        videos = list(self.videos_source.find(query, projection).limit(limit or 0))
        
        # Count videos by processing status for better logging
        if videos:
            never_processed = len([v for v in videos if not v.get("graph_processed")])
            wrong_version = len([v for v in videos if v.get("graph_processed") and 
                               v.get("graph_processing_version") != current_version])
            no_version = len([v for v in videos if v.get("graph_processed") and 
                            not v.get("graph_processing_version")])
            
            print(f"Found {len(videos)} videos to process:")
            if never_processed > 0:
                print(f"  - {never_processed} never processed")
            if wrong_version > 0:
                print(f"  - {wrong_version} with outdated version")
            if no_version > 0:
                print(f"  - {no_version} missing version info")
        
        if video_id and not videos:
            print(f"No video found with ID: {video_id} or already processed with current version")
        elif not video_id and not videos:
            print("All videos with JSON-LD data are already processed with current version")
        
        return videos
    
    def expand_curie(self, curie: str, context: Dict[str, Any]) -> str:
        """
        Expand a CURIE (Compact URI) to a full IRI using the JSON-LD context.
        
        Args:
            curie: The compact URI to expand (e.g., "bbp:Program_DigitalMedia")
            context: The JSON-LD @context dictionary
            
        Returns:
            Expanded IRI or original string if not expandable
        """
        if not curie or not isinstance(curie, str):
            return curie
        
        # Already a full IRI
        if curie.startswith(("http://", "https://")):
            return curie
        
        # Blank node
        if curie.startswith("_:"):
            return curie
        
        # Check if it contains a colon (potential CURIE)
        if ":" in curie:
            prefix, local_part = curie.split(":", 1)
            
            # Look up prefix in context
            if prefix in context:
                base_iri = context[prefix]
                
                # Handle different context value types
                if isinstance(base_iri, str):
                    # Simple string mapping
                    if base_iri.endswith(("#", "/")):
                        return base_iri + local_part
                    else:
                        return base_iri + local_part
                elif isinstance(base_iri, dict):
                    # Complex context object
                    if "@id" in base_iri:
                        base = base_iri["@id"]
                        if base.endswith(("#", "/")):
                            return base + local_part
                        else:
                            return base + local_part
        
        # If we can't expand it, return as-is
        return curie
    
    def expand_value_recursively(self, value: Any, context: Dict[str, Any]) -> Any:
        """
        Recursively expand CURIEs in a value structure.
        
        Args:
            value: Value to expand (can be string, dict, list, etc.)
            context: JSON-LD context for expansion
            
        Returns:
            Value with CURIEs expanded
        """
        if isinstance(value, str):
            return self.expand_curie(value, context)
        elif isinstance(value, dict):
            if "@id" in value:
                # This is a reference object
                expanded_id = self.expand_curie(value["@id"], context)
                result = dict(value)
                result["@id"] = expanded_id
                return result
            else:
                # Regular object, expand all values
                return {k: self.expand_value_recursively(v, context) for k, v in value.items()}
        elif isinstance(value, list):
            return [self.expand_value_recursively(item, context) for item in value]
        else:
            return value
    
    def extract_local_name_from_iri(self, iri: str) -> str:
        """Extract local name from a full IRI for display purposes."""
        if '#' in iri:
            return iri.split('#')[-1]
        elif '/' in iri:
            return iri.split('/')[-1]
        else:
            return iri
    
    def extract_label_from_properties(self, node: Dict[str, Any]) -> Optional[str]:
        """
        Extract the best label from JSON-LD node properties.
        Priority order: schema:name, rdfs:label, foaf:name, dcterms:title, schema:title
        """
        # Define common label/name/title properties in priority order (now with full IRIs)
        label_properties = [
            "http://schema.org/name",
            "http://www.w3.org/2000/01/rdf-schema#label",
            "http://xmlns.com/foaf/0.1/name",
            "http://purl.org/dc/terms/title",
            "http://schema.org/title",
            "http://www.w3.org/2004/02/skos/core#prefLabel",
            # Keep compact forms as fallback
            "schema:name",
            "rdfs:label",
            "foaf:name",
            "dcterms:title",
            "schema:title",
            "skos:prefLabel",
        ]
        
        for prop in label_properties:
            if prop in node:
                value = node[prop]
                # Handle both single values and arrays
                if isinstance(value, list):
                    return str(value[0]) if value else None
                elif isinstance(value, dict) and "@value" in value:
                    return str(value["@value"])
                else:
                    return str(value)
        
        return None
    
    def create_searchable_text(self, node: Dict[str, Any], node_types: List[str]) -> str:
        """
        Create a comprehensive searchable text field from all node information.
        """
        text_parts = []
        uri_str = node.get("@id", "")
        
        # Add label/name (highest priority)
        label = self.extract_label_from_properties(node)
        if label:
            text_parts.append(label)
        
        # Add local name from IRI (if different from label)
        if uri_str:
            local_name = self.extract_local_name_from_iri(uri_str)
            if local_name and local_name != label:
                # Convert camelCase to readable text
                readable_local = re.sub(r'([a-z])([A-Z])', r'\1 \2', local_name)
                text_parts.append(readable_local)
        
        # Add all name/title/label properties from the JSON-LD
        name_properties = [
            "http://schema.org/name",
            "http://schema.org/title",
            "http://www.w3.org/2000/01/rdf-schema#label",
            "http://xmlns.com/foaf/0.1/name",
            "http://purl.org/dc/terms/title",
            "http://www.w3.org/2004/02/skos/core#prefLabel",
            "http://example.com/barbados-parliament-ontology#hasRole",
            # Compact forms as fallback
            "schema:name",
            "schema:title",
            "rdfs:label",
            "foaf:name",
            "dcterms:title",
            "skos:prefLabel",
            "bbp:hasRole"
        ]
        
        for prop in name_properties:
            if prop in node:
                value = node[prop]
                if isinstance(value, list):
                    for v in value:
                        text_val = v.get("@value", v) if isinstance(v, dict) else str(v)
                        if text_val and str(text_val).strip() and str(text_val) not in text_parts:
                            text_parts.append(str(text_val).strip())
                elif isinstance(value, dict):
                    text_val = value.get("@value", str(value))
                    if text_val and str(text_val).strip() and str(text_val) not in text_parts:
                        text_parts.append(str(text_val).strip())
                else:
                    if value and str(value).strip() and str(value) not in text_parts:
                        text_parts.append(str(value).strip())
        
        # Add descriptive properties
        descriptive_properties = [
            "http://schema.org/description",
            "http://purl.org/dc/terms/description",
            "http://www.w3.org/2000/01/rdf-schema#comment",
            # Compact forms as fallback
            "schema:description",
            "dcterms:description",
            "rdfs:comment"
        ]
        
        for prop in descriptive_properties:
            if prop in node:
                value = node[prop]
                if isinstance(value, list):
                    for v in value:
                        text_val = v.get("@value", v) if isinstance(v, dict) else str(v)
                        if text_val and str(text_val).strip():
                            text_parts.append(str(text_val).strip())
                elif isinstance(value, dict):
                    text_val = value.get("@value", str(value))
                    if text_val and str(text_val).strip():
                        text_parts.append(str(text_val).strip())
                else:
                    if value and str(value).strip():
                        text_parts.append(str(value).strip())
        
        # Add readable type names (but not generic ones)
        for node_type in node_types:
            type_name = self.extract_local_name_from_iri(node_type)
            if type_name and type_name not in ['Entity', 'Thing']:
                # Convert camelCase to readable text
                readable_type = re.sub(r'([a-z])([A-Z])', r'\1 \2', type_name)
                text_parts.append(readable_type)
        
        # Clean up and deduplicate
        clean_parts = []
        seen = set()
        
        for part in text_parts:
            if part and isinstance(part, str):
                # Basic cleaning
                clean_part = part.strip()
                
                # Skip very short or generic parts
                if len(clean_part) < 2 or clean_part.lower() in ['entity', 'thing', 'object']:
                    continue
                
                # Convert to lowercase for deduplication check
                clean_lower = clean_part.lower()
                if clean_lower not in seen:
                    seen.add(clean_lower)
                    clean_parts.append(clean_part)
        
        return ' '.join(clean_parts)
    
    def generate_embedding(self, text: str) -> Optional[List[float]]:
        """Generate vector embedding for text content."""
        if not self.use_embeddings or not text:
            return None
        
        try:
            # Clean text for embedding
            clean_text = re.sub(r'\s+', ' ', text).strip()
            if len(clean_text) < 3:  # Skip very short text
                return None
            
            embedding = self.embedding_model.encode(clean_text)
            return embedding.tolist()
        except Exception as e:
            print(f"⚠️  Failed to generate embedding: {e}")
            return None
    
    def get_node_types(self, node: Dict[str, Any]) -> List[str]:
        """Extract types from JSON-LD node."""
        types = []
        if "@type" in node:
            node_type = node["@type"]
            if isinstance(node_type, list):
                types.extend([str(t) for t in node_type])
            else:
                types.append(str(node_type))
        
        # If no explicit type, try to infer
        if not types:
            uri_str = node.get("@id", "")
            if 'Person' in uri_str or 'foaf' in uri_str:
                types.append("http://xmlns.com/foaf/0.1/Person")
            elif 'Concept' in uri_str:
                types.append("http://example.com/barbados-parliament-ontology#Concept")
            elif 'Statement' in uri_str:
                types.append("http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement")
            else:
                types.append("http://example.org/Entity")
        
        return types
    
    def extract_properties_from_jsonld_node(self, node: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """Extract properties from JSON-LD node, excluding @id and @type, and expand CURIEs."""
        properties = {}
        
        for key, value in node.items():
            if key.startswith("@"):  # Skip @id, @type, @context, etc.
                continue
            
            # Expand the property key
            expanded_key = self.expand_curie(key, context)
            
            # Expand the value recursively
            expanded_value = self.expand_value_recursively(value, context)
            
            # Handle different value formats
            if isinstance(expanded_value, dict) and "@id" in expanded_value:
                # Reference to another entity
                properties[expanded_key] = expanded_value["@id"]
            elif isinstance(expanded_value, dict) and "@value" in expanded_value:
                # Typed literal
                properties[expanded_key] = expanded_value["@value"]
            elif isinstance(expanded_value, list):
                # Array of values
                processed_values = []
                for v in expanded_value:
                    if isinstance(v, dict) and "@id" in v:
                        processed_values.append(v["@id"])
                    elif isinstance(v, dict) and "@value" in v:
                        processed_values.append(v["@value"])
                    else:
                        processed_values.append(v)
                properties[expanded_key] = processed_values if len(processed_values) > 1 else processed_values[0]
            else:
                # Direct value
                properties[expanded_key] = expanded_value
        
        return properties
    
    def create_statement_id(self, subject: str, predicate: str, object_val: str) -> str:
        """Create a unique ID for a statement."""
        content = f"{subject}|{predicate}|{object_val}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def safe_float_conversion(self, value, default=0.0) -> float:
        """
        Safely convert a value to float, handling empty strings and None values.
        
        Args:
            value: The value to convert to float
            default: Default value to return if conversion fails
            
        Returns:
            float: Converted value or default
        """
        if value is None:
            return default
        
        if isinstance(value, (int, float)):
            return float(value)
        
        if isinstance(value, str):
            value = value.strip()
            if not value:  # Empty string
                return default
            try:
                return float(value)
            except ValueError:
                return default
        
        return default
    
    def cleanup_old_graph_data(self, video_id: str) -> bool:
        """
        Clean up old graph data for a video before reprocessing.
        
        Args:
            video_id: The video ID to clean up data for
            
        Returns:
            True if cleanup was successful, False otherwise
        """
        try:
            # Remove old nodes for this video
            nodes_result = self.nodes.delete_many({"source_video": video_id})
            
            # Remove old edges for this video
            edges_result = self.edges.delete_many({"source_video": video_id})
            
            # Remove old statements for this video
            statements_result = self.statements.delete_many({"source_video": video_id})
            
            print(f"    🧹 Cleaned up old data: {nodes_result.deleted_count} nodes, "
                  f"{edges_result.deleted_count} edges, {statements_result.deleted_count} statements")
            
            return True
            
        except Exception as e:
            print(f"    ⚠️  Failed to cleanup old data: {e}")
            return False

    def process_jsonld_to_graph(self, json_ld: Dict[str, Any], video_id: str, video_title: str):
        """Process JSON-LD data and save to graph collections."""
        print(f"  📊 Processing JSON-LD graph data...")
        
        if "@graph" not in json_ld:
            print("  ⚠️  No @graph found in JSON-LD")
            return
        
        # Extract context for CURIE expansion
        context = json_ld.get("@context", {})
        if not isinstance(context, dict):
            context = {}
        
        graph_items = json_ld["@graph"]
        print(f"  📋 Found {len(graph_items)} graph items")
        print(f"  🔗 Context prefixes: {list(context.keys())}")
        
        nodes_added = 0
        nodes_updated = 0
        edges_added = 0
        statements_added = 0
        embeddings_generated = 0
        
        # Separate different types of items
        entity_nodes = []
        reified_statements = []
        
        for item in graph_items:
            if item.get("@type") == "rdf:Statement":
                reified_statements.append(item)
            else:
                entity_nodes.append(item)
        
        print(f"    - Entity nodes: {len(entity_nodes)}")
        print(f"    - Reified statements: {len(reified_statements)}")
        
        # Process entity nodes
        for node in entity_nodes:
            if "@id" not in node:
                continue
            
            # Expand the URI
            uri_str = self.expand_curie(node["@id"], context)
            node_types = [self.expand_curie(t, context) for t in self.get_node_types(node)]
            properties = self.extract_properties_from_jsonld_node(node, context)
            
            # Extract label
            label = self.extract_label_from_properties(node)
            if not label:
                label = self.extract_local_name_from_iri(uri_str)
            
            # Create searchable text
            searchable_text = self.create_searchable_text(node, node_types)
            
            # Generate embedding
            embedding = None
            if self.use_embeddings and searchable_text:
                embedding = self.generate_embedding(searchable_text)
                if embedding:
                    embeddings_generated += 1
            
            node_doc = {
                "uri": uri_str,
                "label": label,
                "searchable_text": searchable_text,
                "type": node_types,
                "properties": properties,
                "source_video": [video_id],
                "video_title": video_title,
                "created_at": datetime.now(timezone.utc),
                "updated_at": datetime.now(timezone.utc)
            }
            
            # Add embedding if generated
            if embedding:
                node_doc["embedding"] = embedding
            
            try:
                self.nodes.insert_one(node_doc)
                nodes_added += 1
            except DuplicateKeyError:
                # Update existing node
                update_doc = {
                    "properties": properties,
                    "label": label,
                    "searchable_text": searchable_text,
                    "updated_at": datetime.now(timezone.utc)
                }
                
                if embedding:
                    update_doc["embedding"] = embedding
                
                self.nodes.update_one(
                    {"uri": uri_str},
                    {
                        "$set": update_doc,
                        "$addToSet": {"source_video": video_id}
                    }
                )
                nodes_updated += 1
            
            # Create edges for this node's properties
            for prop_key, prop_value in properties.items():
                if isinstance(prop_value, str) and prop_value.startswith(("http://", "https://", "_:")):
                    # This is likely a reference to another entity
                    edge_doc = {
                        "subject": uri_str,
                        "predicate": prop_key,
                        "object": prop_value,
                        "source_video": [video_id],
                        "video_title": video_title,
                        "created_at": datetime.now(timezone.utc)
                    }
                    
                    try:
                        self.edges.insert_one(edge_doc)
                        edges_added += 1
                    except DuplicateKeyError:
                        pass  # Skip duplicates
        
        # Process reified statements
        for stmt in reified_statements:
            if "@id" not in stmt:
                continue
            
            # Extract statement components and expand CURIEs
            subject = stmt.get("rdf:subject", {}).get("@id") if isinstance(stmt.get("rdf:subject"), dict) else stmt.get("rdf:subject")
            predicate = stmt.get("rdf:predicate", {}).get("@id") if isinstance(stmt.get("rdf:predicate"), dict) else stmt.get("rdf:predicate")
            object_val = stmt.get("rdf:object")
            
            if subject:
                subject = self.expand_curie(subject, context)
            if predicate:
                predicate = self.expand_curie(predicate, context)
            
            if not (subject and predicate and object_val):
                continue
            
            statement_id = self.create_statement_id(subject, predicate, str(object_val))
            
            statement_doc = {
                "statement_id": statement_id,
                "global_statement_id": f"{video_id}_{statement_id}",
                "statement_uri": self.expand_curie(stmt["@id"], context),
                "subject": subject,
                "predicate": predicate,
                "object": str(object_val),
                "source_video": video_id,
                "video_title": video_title,
                "created_at": datetime.now(timezone.utc)
            }
            
            # Extract provenance information with safe float conversion
            provenance = stmt.get("prov:wasDerivedFrom")
            if provenance:
                if isinstance(provenance, dict):
                    # Safely extract from_video
                    from_video = provenance.get("bbp:fromVideo")
                    if isinstance(from_video, dict):
                        from_video = from_video.get("@id")
                    if from_video:
                        from_video = self.expand_curie(from_video, context)
                    
                    # Safely extract start_offset
                    start_offset_data = provenance.get("bbp:startTimeOffset")
                    if isinstance(start_offset_data, dict):
                        start_offset = self.safe_float_conversion(start_offset_data.get("@value", 0))
                    else:
                        start_offset = self.safe_float_conversion(start_offset_data)
                    
                    # Safely extract end_offset
                    end_offset_data = provenance.get("bbp:endTimeOffset")
                    if isinstance(end_offset_data, dict):
                        end_offset = self.safe_float_conversion(end_offset_data.get("@value", 0))
                    else:
                        end_offset = self.safe_float_conversion(end_offset_data)
                    
                    statement_doc.update({
                        "from_video": from_video,
                        "start_offset": start_offset,
                        "end_offset": end_offset,
                        "transcript_text": provenance.get("bbp:transcriptText"),
                        "segment_type": provenance.get("@type")
                    })
            
            try:
                self.statements.insert_one(statement_doc)
                statements_added += 1
            except DuplicateKeyError:
                pass  # Skip duplicates
        
        print(f"  ✅ Graph processing complete:")
        print(f"    - Nodes: {nodes_added} added, {nodes_updated} updated")
        print(f"    - Edges: {edges_added} added")
        print(f"    - Statements: {statements_added} added")
        if self.use_embeddings:
            print(f"    - Embeddings: {embeddings_generated} generated")
        """Process JSON-LD data and save to graph collections."""
        print(f"  📊 Processing JSON-LD graph data...")
        
        if "@graph" not in json_ld:
            print("  ⚠️  No @graph found in JSON-LD")
            return
        
        # Extract context for CURIE expansion
        context = json_ld.get("@context", {})
        if not isinstance(context, dict):
            context = {}
        
        graph_items = json_ld["@graph"]
        print(f"  📋 Found {len(graph_items)} graph items")
        print(f"  🔗 Context prefixes: {list(context.keys())}")
        
        nodes_added = 0
        nodes_updated = 0
        edges_added = 0
        statements_added = 0
        embeddings_generated = 0
        
        # Separate different types of items
        entity_nodes = []
        reified_statements = []
        
        for item in graph_items:
            if item.get("@type") == "rdf:Statement":
                reified_statements.append(item)
            else:
                entity_nodes.append(item)
        
        print(f"    - Entity nodes: {len(entity_nodes)}")
        print(f"    - Reified statements: {len(reified_statements)}")
        
        # Process entity nodes
        for node in entity_nodes:
            if "@id" not in node:
                continue
            
            # Expand the URI
            uri_str = self.expand_curie(node["@id"], context)
            node_types = [self.expand_curie(t, context) for t in self.get_node_types(node)]
            properties = self.extract_properties_from_jsonld_node(node, context)
            
            # Extract label
            label = self.extract_label_from_properties(node)
            if not label:
                label = self.extract_local_name_from_iri(uri_str)
            
            # Create searchable text
            searchable_text = self.create_searchable_text(node, node_types)
            
            # Generate embedding
            embedding = None
            if self.use_embeddings and searchable_text:
                embedding = self.generate_embedding(searchable_text)
                if embedding:
                    embeddings_generated += 1
            
            node_doc = {
                "uri": uri_str,
                "label": label,
                "searchable_text": searchable_text,
                "type": node_types,
                "properties": properties,
                "source_video": [video_id],
                "video_title": video_title,
                "created_at": datetime.now(timezone.utc),
                "updated_at": datetime.now(timezone.utc)
            }
            
            # Add embedding if generated
            if embedding:
                node_doc["embedding"] = embedding
            
            try:
                self.nodes.insert_one(node_doc)
                nodes_added += 1
            except DuplicateKeyError:
                # Update existing node
                update_doc = {
                    "properties": properties,
                    "label": label,
                    "searchable_text": searchable_text,
                    "updated_at": datetime.now(timezone.utc)
                }
                
                if embedding:
                    update_doc["embedding"] = embedding
                
                self.nodes.update_one(
                    {"uri": uri_str},
                    {
                        "$set": update_doc,
                        "$addToSet": {"source_video": video_id}
                    }
                )
                nodes_updated += 1
            
            # Create edges for this node's properties
            for prop_key, prop_value in properties.items():
                if isinstance(prop_value, str) and prop_value.startswith(("http://", "https://", "_:")):
                    # This is likely a reference to another entity
                    edge_doc = {
                        "subject": uri_str,
                        "predicate": prop_key,
                        "object": prop_value,
                        "source_video": [video_id],
                        "video_title": video_title,
                        "created_at": datetime.now(timezone.utc)
                    }
                    
                    try:
                        self.edges.insert_one(edge_doc)
                        edges_added += 1
                    except DuplicateKeyError:
                        pass  # Skip duplicates
        
        # Process reified statements
        for stmt in reified_statements:
            if "@id" not in stmt:
                continue
            
            # Extract statement components and expand CURIEs
            subject = stmt.get("rdf:subject", {}).get("@id") if isinstance(stmt.get("rdf:subject"), dict) else stmt.get("rdf:subject")
            predicate = stmt.get("rdf:predicate", {}).get("@id") if isinstance(stmt.get("rdf:predicate"), dict) else stmt.get("rdf:predicate")
            object_val = stmt.get("rdf:object")
            
            if subject:
                subject = self.expand_curie(subject, context)
            if predicate:
                predicate = self.expand_curie(predicate, context)
            
            if not (subject and predicate and object_val):
                continue
            
            statement_id = self.create_statement_id(subject, predicate, str(object_val))
            
            statement_doc = {
                "statement_id": statement_id,
                "global_statement_id": f"{video_id}_{statement_id}",
                "statement_uri": self.expand_curie(stmt["@id"], context),
                "subject": subject,
                "predicate": predicate,
                "object": str(object_val),
                "source_video": video_id,
                "video_title": video_title,
                "created_at": datetime.now(timezone.utc)
            }
            
            # Extract provenance information with safe float conversion
            provenance = stmt.get("prov:wasDerivedFrom")
            if provenance:
                if isinstance(provenance, dict):
                    # Safely extract from_video
                    from_video = provenance.get("bbp:fromVideo")
                    if isinstance(from_video, dict):
                        from_video = from_video.get("@id")
                    if from_video:
                        from_video = self.expand_curie(from_video, context)
                    
                    # Safely extract start_offset
                    start_offset_data = provenance.get("bbp:startTimeOffset")
                    if isinstance(start_offset_data, dict):
                        start_offset = self.safe_float_conversion(start_offset_data.get("@value", 0))
                    else:
                        start_offset = self.safe_float_conversion(start_offset_data)
                    
                    # Safely extract end_offset
                    end_offset_data = provenance.get("bbp:endTimeOffset")
                    if isinstance(end_offset_data, dict):
                        end_offset = self.safe_float_conversion(end_offset_data.get("@value", 0))
                    else:
                        end_offset = self.safe_float_conversion(end_offset_data)
                    
                    statement_doc.update({
                        "from_video": from_video,
                        "start_offset": start_offset,
                        "end_offset": end_offset,
                        "transcript_text": provenance.get("bbp:transcriptText"),
                        "segment_type": provenance.get("@type")
                    })
            
            try:
                self.statements.insert_one(statement_doc)
                statements_added += 1
            except DuplicateKeyError:
                pass  # Skip duplicates
        
        print(f"  ✅ Graph processing complete:")
        print(f"    - Nodes: {nodes_added} added, {nodes_updated} updated")
        print(f"    - Edges: {edges_added} added")
        print(f"    - Statements: {statements_added} added")
        if self.use_embeddings:
            print(f"    - Embeddings: {embeddings_generated} generated")
    
    def save_graph_video_metadata(self, video_data: Dict[str, Any]):
        """Mark video as processed by adding graph_processed flag."""
        try:
            self.videos_source.update_one(
                {"video_id": video_data["video_id"]},
                {
                    "$set": {
                        "graph_processed": True,
                        "graph_processed_at": datetime.now(timezone.utc),
                        "graph_processing_version": "mongodb_graph_loader_v2.0"
                    }
                }
            )
            print(f"    ✅ Marked video as graph processed: {video_data['video_id']}")
            return True
        except Exception as e:
            print(f"    ⚠️  Failed to mark video as processed: {e}")
            return False
    
    def process_video(self, video_data: Dict[str, Any]) -> bool:
        """Process a single video's JSON-LD data into graph format."""
        video_id = video_data["video_id"]
        video_title = video_data.get("Video_title", "Unknown Title")
        json_ld = video_data.get("json_ld")
        current_version = "mongodb_graph_loader_v2.0"
        
        # Check if this is a reprocessing scenario
        is_reprocessing = video_data.get("graph_processed", False) and \
                         video_data.get("graph_processing_version") != current_version
        
        if not json_ld:
            print(f"  ⚠️  No JSON-LD data found for video {video_id}")
            return False
        
        try:
            print(f"  📄 JSON-LD data: {len(str(json_ld))} characters")
            
            # Clean up old data if reprocessing
            if is_reprocessing:
                print(f"  🔄 Reprocessing due to version change")
                if not self.cleanup_old_graph_data(video_id):
                    print(f"  ⚠️  Cleanup failed, continuing anyway...")
            
            # Process JSON-LD into graph structure
            self.process_jsonld_to_graph(json_ld, video_id, video_title)
            
            # Mark as processed
            if self.save_graph_video_metadata(video_data):
                return True
            else:
                return False
            
        except Exception as e:
            print(f"  ❌ Error processing video {video_id}: {e}")
            return False
    
    def process_all_videos(self, limit: Optional[int] = None, video_id: Optional[str] = None):
        """
        Process all videos with JSON-LD data into graph format.
        
        Args:
            limit: Maximum number of videos to process
            video_id: Process only a specific video ID
        """
        print("Starting graph loading from JSON-LD data in MongoDB...")
        
        # Get videos with JSON-LD data
        videos_to_process = self.get_videos_with_jsonld(limit=limit, video_id=video_id)
        
        if not videos_to_process:
            print("No videos to process")
            return
        
        stats = {
            "total": len(videos_to_process),
            "processed": 0,
            "errors": 0
        }
        
        for i, video in enumerate(videos_to_process, 1):
            video_id = video["video_id"]
            video_title = video.get("Video_title", "Unknown Title")
            
            print(f"\n[{i}/{stats['total']}] Processing: {video_title[:80]}...")
            print(f"  🆔 Video ID: {video_id}")
            
            # Process the video
            if self.process_video(video):
                stats["processed"] += 1
            else:
                stats["errors"] += 1
        
        # Print final statistics
        print(f"\n📊 Graph Loading Complete!")
        print(f"  Total videos: {stats['total']}")
        print(f"  Successfully processed: {stats['processed']}")
        print(f"  Errors: {stats['errors']}")
    
    def get_stats(self) -> Dict[str, int]:
        """Get statistics about the loaded graph."""
        current_version = "mongodb_graph_loader_v2.0"
        
        stats = {
            "nodes": self.nodes.count_documents({}),
            "edges": self.edges.count_documents({}), 
            "statements": self.statements.count_documents({}),
            "videos_with_jsonld": self.videos_source.count_documents({"json_ld": {"$exists": True}}),
            "videos_graph_processed": self.videos_source.count_documents({"graph_processed": True}),
            "videos_current_version": self.videos_source.count_documents({
                "graph_processed": True,
                "graph_processing_version": current_version
            }),
            "videos_outdated_version": self.videos_source.count_documents({
                "graph_processed": True,
                "$or": [
                    {"graph_processing_version": {"$ne": current_version}},
                    {"graph_processing_version": {"$exists": False}}
                ]
            })
        }
        
        if self.use_embeddings:
            stats["nodes_with_embeddings"] = self.nodes.count_documents({"embedding": {"$exists": True}})
        
        return stats

def main():
    """Main function to run the script."""
    parser = argparse.ArgumentParser(description="Load JSON-LD data from MongoDB into graph collections with vector search support")
    parser.add_argument("--database", default="parliamentary_graph", help="MongoDB database name")
    parser.add_argument("--limit", type=int, help="Limit number of videos to process")
    parser.add_argument("--video-id", help="Process only a specific video ID")
    parser.add_argument("--skip-embeddings", action="store_true", 
                        help="Skip generating vector embeddings (faster processing)")
    parser.add_argument("--stats", action="store_true", help="Show graph statistics only")
    
    args = parser.parse_args()
    
    try:
        # Initialize loader
        loader = MongoDBGraphLoader(
            database_name=args.database,
            use_embeddings=not args.skip_embeddings
        )
        
        if args.stats:
            # Show statistics only
            stats = loader.get_stats()
            print("📊 Graph Statistics:")
            for collection, count in stats.items():
                print(f"  {collection}: {count:,} documents")
            return
        
        # Process videos
        loader.process_all_videos(
            limit=args.limit,
            video_id=args.video_id
        )
        
        # Show final statistics
        stats = loader.get_stats()
        print(f"\n📊 Final Graph Statistics:")
        for collection, count in stats.items():
            print(f"  {collection}: {count:,} documents")
        
    except ValueError as e:
        print(f"Configuration error: {e}")
        print("\nTo set up MongoDB connection:")
        print("1. Create a MongoDB Atlas cluster: https://cloud.mongodb.com/")
        print("2. Get your connection string from the Atlas dashboard")
        print("3. Set environment variable: export MONGODB_CONNECTION_STRING='your-connection-string'")
        print("4. Or create a .env file with: MONGODB_CONNECTION_STRING=your-connection-string")
        sys.exit(1)
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()