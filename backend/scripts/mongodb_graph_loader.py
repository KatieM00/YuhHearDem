#!/usr/bin/env python3
"""
MongoDB JSON-LD to Graph Loader with Vector Search Support

This script loads JSON-LD data from MongoDB (generated by mongodb_ttl_generator.py) 
and saves them as graph structures in MongoDB Atlas for GraphRAG-style querying 
and analysis, with support for vector embeddings.

Requirements:
- pymongo
- rdflib
- python-dotenv (optional, for environment variables)
- sentence-transformers (for generating embeddings)

Usage:
    python mongodb_graph_loader.py --database parliamentary_graph [--skip-embeddings] [--limit N] [--video-id VIDEO_ID]
"""

import sys
import os
import json
import argparse
from typing import Dict, List, Any, Optional
from urllib.parse import urlparse
import hashlib
from datetime import datetime, timezone
import re

try:
    from pymongo import MongoClient, ASCENDING, TEXT
    from pymongo.errors import ConnectionFailure, DuplicateKeyError
    from rdflib import Graph, URIRef, Literal, BNode
    from rdflib.namespace import RDF, RDFS, OWL, FOAF, XSD
    from dotenv import load_dotenv
except ImportError as e:
    print(f"Missing required package: {e}")
    print("Please install required packages:")
    print("pip install pymongo rdflib python-dotenv")
    sys.exit(1)

# Optional: Import sentence transformers for embeddings
try:
    from sentence_transformers import SentenceTransformer
    EMBEDDINGS_AVAILABLE = True
except ImportError:
    EMBEDDINGS_AVAILABLE = False
    print("âš ï¸  sentence-transformers not available. Install with: pip install sentence-transformers")

# Load environment variables
load_dotenv()

class MongoDBGraphLoader:
    def __init__(self, connection_string: str = None, database_name: str = "parliamentary_graph", 
                 use_embeddings: bool = True):
        """
        Initialize the MongoDB graph loader.
        
        Args:
            connection_string: MongoDB Atlas connection string. If None, will try to get from environment.
            database_name: Name of the MongoDB database to use
            use_embeddings: Whether to generate vector embeddings for text content
        """
        if connection_string is None:
            connection_string = os.getenv('MONGODB_CONNECTION_STRING')
            
        if not connection_string:
            raise ValueError(
                "MongoDB connection string is required. Set MONGODB_CONNECTION_STRING environment variable "
                "or pass connection_string parameter."
            )
        
        try:
            self.client = MongoClient(connection_string)
            # Test connection
            self.client.admin.command('ping')
            print("âœ… Successfully connected to MongoDB Atlas")
        except ConnectionFailure as e:
            raise ConnectionFailure(f"Failed to connect to MongoDB: {e}")
        
        self.db = self.client[database_name]
        self.use_embeddings = use_embeddings and EMBEDDINGS_AVAILABLE
        
        # Source collection (videos with JSON-LD data)
        self.videos_source = self.db.videos
        
        # Initialize embedding model if available
        if self.use_embeddings:
            try:
                print("ðŸ”„ Loading embedding model...")
                self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
                print("âœ… Embedding model loaded successfully")
            except Exception as e:
                print(f"âš ï¸  Failed to load embedding model: {e}")
                self.use_embeddings = False
        
        self.setup_collections()
    
    def setup_collections(self):
        """Set up MongoDB collections with appropriate indexes."""
        
        # Nodes collection for entities
        self.nodes = self.db.nodes
        try:
            self.nodes.create_index([("uri", ASCENDING)], unique=True)
        except:
            pass  # Index already exists
        try:
            self.nodes.create_index([("type", ASCENDING)])
        except:
            pass
        try:
            self.nodes.create_index([("label", TEXT), ("searchable_text", TEXT)])
        except:
            pass
        try:
            self.nodes.create_index([("source_video", ASCENDING)])
        except:
            pass
        
        # Edges collection for relationships
        self.edges = self.db.edges
        try:
            self.edges.create_index([("subject", ASCENDING)])
        except:
            pass
        try:
            self.edges.create_index([("predicate", ASCENDING)])
        except:
            pass
        try:
            self.edges.create_index([("object", ASCENDING)])
        except:
            pass
        try:
            self.edges.create_index([("source_video", ASCENDING)])
        except:
            pass
        try:
            self.edges.create_index([("subject", ASCENDING), ("predicate", ASCENDING), ("object", ASCENDING)], unique=True)
        except:
            pass
        
        # Statements collection for reified statements (provenance)
        self.statements = self.db.statements
        
        # Drop and recreate the problematic index
        try:
            self.statements.drop_index("statement_id_1")
        except:
            pass  # Index doesn't exist
        
        try:
            self.statements.create_index([("global_statement_id", ASCENDING)], unique=True)
        except:
            pass
        try:
            self.statements.create_index([("statement_id", ASCENDING)])
        except:
            pass
        try:
            self.statements.create_index([("subject", ASCENDING)])
        except:
            pass
        try:
            self.statements.create_index([("predicate", ASCENDING)])
        except:
            pass
        try:
            self.statements.create_index([("object", ASCENDING)])
        except:
            pass
        try:
            self.statements.create_index([("source_video", ASCENDING)])
        except:
            pass
        try:
            self.statements.create_index([("start_offset", ASCENDING)])
        except:
            pass
        try:
            self.statements.create_index([("end_offset", ASCENDING)])
        except:
            pass
        
        print("âœ… Collections and indexes set up successfully")
        
        if self.use_embeddings:
            print("â„¹ï¸  Vector search indexes need to be created manually in Atlas:")
            print("   1. Go to your Atlas cluster")
            print("   2. Navigate to Search > Create Search Index")
            print("   3. Choose 'Vector Search' and use the nodes collection")
            print("   4. Use this configuration:")
            print("""   {
     "fields": [
       {
         "type": "vector",
         "path": "embedding",
         "numDimensions": 384,
         "similarity": "cosine"
       }
     ]
   }""")
    
    def get_videos_with_jsonld(self, limit: Optional[int] = None, video_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        Get videos with JSON-LD data that haven't been processed into graph format yet.
        
        Args:
            limit: Maximum number of videos to process
            video_id: Process only a specific video ID
            
        Returns:
            List of video documents with JSON-LD data
        """
        # Build query
        query = {
            "json_ld": {"$exists": True, "$ne": None},
            "graph_processed": {"$ne": True}  # Only videos not yet processed into graph
        }
        
        if video_id:
            query["video_id"] = video_id
        
        # Only fetch necessary fields
        projection = {
            "video_id": 1,
            "Video_title": 1,
            "VideoURL": 1,
            "json_ld": 1,
            "rdf_triple_count": 1,
            "_id": 1
        }
        
        videos = list(self.videos_source.find(query, projection).limit(limit or 0))
        
        if video_id and not videos:
            print(f"No video found with ID: {video_id} or already processed")
        else:
            print(f"Found {len(videos)} videos with JSON-LD data to process into graph format")
        
        return videos
    
    def extract_local_name(self, uri_str: str) -> str:
        """Extract local name from URI for labeling."""
        if '#' in uri_str:
            return uri_str.split('#')[-1]
        elif '/' in uri_str:
            return uri_str.split('/')[-1]
        else:
            return uri_str
    
    def extract_label_from_properties(self, node: Dict[str, Any]) -> Optional[str]:
        """
        Extract the best label from JSON-LD node properties.
        Priority order: schema:name, rdfs:label, foaf:name, dcterms:title, schema:title
        """
        # Define common label/name/title properties in priority order
        label_properties = [
            "schema:name",
            "http://schema.org/name",
            "rdfs:label",
            "http://www.w3.org/2000/01/rdf-schema#label",
            "foaf:name", 
            "http://xmlns.com/foaf/0.1/name",
            "dcterms:title",
            "http://purl.org/dc/terms/title",
            "schema:title",
            "http://schema.org/title",
            "skos:prefLabel",
            "http://www.w3.org/2004/02/skos/core#prefLabel",
        ]
        
        for prop in label_properties:
            if prop in node:
                value = node[prop]
                # Handle both single values and arrays
                if isinstance(value, list):
                    return str(value[0]) if value else None
                elif isinstance(value, dict) and "@value" in value:
                    return str(value["@value"])
                else:
                    return str(value)
        
        return None
    
    def create_searchable_text(self, node: Dict[str, Any], node_types: List[str]) -> str:
        """
        Create a comprehensive searchable text field from all node information.
        """
        text_parts = []
        uri_str = node.get("@id", "")
        
        # Add label/name (highest priority)
        label = self.extract_label_from_properties(node)
        if label:
            text_parts.append(label)
        
        # Add local name from URI (if different from label)
        if uri_str:
            local_name = self.extract_local_name(uri_str)
            if local_name and local_name != label:
                # Convert camelCase to readable text
                readable_local = re.sub(r'([a-z])([A-Z])', r'\1 \2', local_name)
                text_parts.append(readable_local)
        
        # Add all name/title/label properties from the JSON-LD
        name_properties = [
            "schema:name", "http://schema.org/name",
            "schema:title", "http://schema.org/title",
            "rdfs:label", "http://www.w3.org/2000/01/rdf-schema#label",
            "foaf:name", "http://xmlns.com/foaf/0.1/name",
            "dcterms:title", "http://purl.org/dc/terms/title",
            "skos:prefLabel", "http://www.w3.org/2004/02/skos/core#prefLabel",
            "bbp:hasRole", "http://example.com/barbados-parliament-ontology#hasRole"
        ]
        
        for prop in name_properties:
            if prop in node:
                value = node[prop]
                if isinstance(value, list):
                    for v in value:
                        text_val = v.get("@value", v) if isinstance(v, dict) else str(v)
                        if text_val and str(text_val).strip() and str(text_val) not in text_parts:
                            text_parts.append(str(text_val).strip())
                elif isinstance(value, dict):
                    text_val = value.get("@value", str(value))
                    if text_val and str(text_val).strip() and str(text_val) not in text_parts:
                        text_parts.append(str(text_val).strip())
                else:
                    if value and str(value).strip() and str(value) not in text_parts:
                        text_parts.append(str(value).strip())
        
        # Add descriptive properties
        descriptive_properties = [
            "schema:description", "http://schema.org/description",
            "dcterms:description", "http://purl.org/dc/terms/description",
            "rdfs:comment", "http://www.w3.org/2000/01/rdf-schema#comment"
        ]
        
        for prop in descriptive_properties:
            if prop in node:
                value = node[prop]
                if isinstance(value, list):
                    for v in value:
                        text_val = v.get("@value", v) if isinstance(v, dict) else str(v)
                        if text_val and str(text_val).strip():
                            text_parts.append(str(text_val).strip())
                elif isinstance(value, dict):
                    text_val = value.get("@value", str(value))
                    if text_val and str(text_val).strip():
                        text_parts.append(str(text_val).strip())
                else:
                    if value and str(value).strip():
                        text_parts.append(str(value).strip())
        
        # Add readable type names (but not generic ones)
        for node_type in node_types:
            type_name = self.extract_local_name(node_type)
            if type_name and type_name not in ['Entity', 'Thing']:
                # Convert camelCase to readable text
                readable_type = re.sub(r'([a-z])([A-Z])', r'\1 \2', type_name)
                text_parts.append(readable_type)
        
        # Clean up and deduplicate
        clean_parts = []
        seen = set()
        
        for part in text_parts:
            if part and isinstance(part, str):
                # Basic cleaning
                clean_part = part.strip()
                
                # Skip very short or generic parts
                if len(clean_part) < 2 or clean_part.lower() in ['entity', 'thing', 'object']:
                    continue
                
                # Convert to lowercase for deduplication check
                clean_lower = clean_part.lower()
                if clean_lower not in seen:
                    seen.add(clean_lower)
                    clean_parts.append(clean_part)
        
        return ' '.join(clean_parts)
    
    def generate_embedding(self, text: str) -> Optional[List[float]]:
        """Generate vector embedding for text content."""
        if not self.use_embeddings or not text:
            return None
        
        try:
            # Clean text for embedding
            clean_text = re.sub(r'\s+', ' ', text).strip()
            if len(clean_text) < 3:  # Skip very short text
                return None
            
            embedding = self.embedding_model.encode(clean_text)
            return embedding.tolist()
        except Exception as e:
            print(f"âš ï¸  Failed to generate embedding: {e}")
            return None
    
    def get_node_types(self, node: Dict[str, Any]) -> List[str]:
        """Extract types from JSON-LD node."""
        types = []
        if "@type" in node:
            node_type = node["@type"]
            if isinstance(node_type, list):
                types.extend([str(t) for t in node_type])
            else:
                types.append(str(node_type))
        
        # If no explicit type, try to infer
        if not types:
            uri_str = node.get("@id", "")
            if 'Person' in uri_str or 'foaf' in uri_str:
                types.append("http://xmlns.com/foaf/0.1/Person")
            elif 'Concept' in uri_str:
                types.append("http://example.com/barbados-parliament-ontology#Concept")
            elif 'Statement' in uri_str:
                types.append("http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement")
            else:
                types.append("http://example.org/Entity")
        
        return types
    
    def extract_properties_from_jsonld_node(self, node: Dict[str, Any]) -> Dict[str, Any]:
        """Extract properties from JSON-LD node, excluding @id and @type."""
        properties = {}
        
        for key, value in node.items():
            if key.startswith("@"):  # Skip @id, @type, @context, etc.
                continue
            
            # Handle different value formats
            if isinstance(value, dict) and "@id" in value:
                # Reference to another entity
                properties[key] = value["@id"]
            elif isinstance(value, dict) and "@value" in value:
                # Typed literal
                properties[key] = value["@value"]
            elif isinstance(value, list):
                # Array of values
                processed_values = []
                for v in value:
                    if isinstance(v, dict) and "@id" in v:
                        processed_values.append(v["@id"])
                    elif isinstance(v, dict) and "@value" in v:
                        processed_values.append(v["@value"])
                    else:
                        processed_values.append(v)
                properties[key] = processed_values if len(processed_values) > 1 else processed_values[0]
            else:
                # Direct value
                properties[key] = value
        
        return properties
    
    def create_statement_id(self, subject: str, predicate: str, object_val: str) -> str:
        """Create a unique ID for a statement."""
        content = f"{subject}|{predicate}|{object_val}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def process_jsonld_to_graph(self, json_ld: Dict[str, Any], video_id: str, video_title: str):
        """Process JSON-LD data and save to graph collections."""
        print(f"  ðŸ“Š Processing JSON-LD graph data...")
        
        if "@graph" not in json_ld:
            print("  âš ï¸  No @graph found in JSON-LD")
            return
        
        graph_items = json_ld["@graph"]
        print(f"  ðŸ“‹ Found {len(graph_items)} graph items")
        
        nodes_added = 0
        nodes_updated = 0
        edges_added = 0
        statements_added = 0
        embeddings_generated = 0
        
        # Separate different types of items
        entity_nodes = []
        reified_statements = []
        
        for item in graph_items:
            if item.get("@type") == "rdf:Statement":
                reified_statements.append(item)
            else:
                entity_nodes.append(item)
        
        print(f"    - Entity nodes: {len(entity_nodes)}")
        print(f"    - Reified statements: {len(reified_statements)}")
        
        # Process entity nodes
        for node in entity_nodes:
            if "@id" not in node:
                continue
            
            uri_str = node["@id"]
            node_types = self.get_node_types(node)
            properties = self.extract_properties_from_jsonld_node(node)
            
            # Extract label
            label = self.extract_label_from_properties(node)
            if not label:
                label = self.extract_local_name(uri_str)
            
            # Create searchable text
            searchable_text = self.create_searchable_text(node, node_types)
            
            # Generate embedding
            embedding = None
            if self.use_embeddings and searchable_text:
                embedding = self.generate_embedding(searchable_text)
                if embedding:
                    embeddings_generated += 1
            
            node_doc = {
                "uri": uri_str,
                "local_name": self.extract_local_name(uri_str),
                "label": label,
                "searchable_text": searchable_text,
                "type": node_types,
                "properties": properties,
                "source_video": [video_id],
                "video_title": video_title,
                "created_at": datetime.now(timezone.utc),
                "updated_at": datetime.now(timezone.utc)
            }
            
            # Add embedding if generated
            if embedding:
                node_doc["embedding"] = embedding
            
            try:
                self.nodes.insert_one(node_doc)
                nodes_added += 1
            except DuplicateKeyError:
                # Update existing node
                update_doc = {
                    "properties": properties,
                    "label": label,
                    "searchable_text": searchable_text,
                    "updated_at": datetime.now(timezone.utc)
                }
                
                if embedding:
                    update_doc["embedding"] = embedding
                
                self.nodes.update_one(
                    {"uri": uri_str},
                    {
                        "$set": update_doc,
                        "$addToSet": {"source_video": video_id}
                    }
                )
                nodes_updated += 1
            
            # Create edges for this node's properties
            for prop_key, prop_value in properties.items():
                if isinstance(prop_value, str) and prop_value.startswith(("http://", "https://", "bbp:", "sess:")):
                    # This is likely a reference to another entity
                    edge_doc = {
                        "subject": uri_str,
                        "predicate": prop_key,
                        "object": prop_value,
                        "source_video": [video_id],
                        "video_title": video_title,
                        "created_at": datetime.now(timezone.utc)
                    }
                    
                    try:
                        self.edges.insert_one(edge_doc)
                        edges_added += 1
                    except DuplicateKeyError:
                        pass  # Skip duplicates
        
        # Process reified statements
        for stmt in reified_statements:
            if "@id" not in stmt:
                continue
            
            # Extract statement components
            subject = stmt.get("rdf:subject", {}).get("@id") if isinstance(stmt.get("rdf:subject"), dict) else stmt.get("rdf:subject")
            predicate = stmt.get("rdf:predicate", {}).get("@id") if isinstance(stmt.get("rdf:predicate"), dict) else stmt.get("rdf:predicate")
            object_val = stmt.get("rdf:object")
            
            if not (subject and predicate and object_val):
                continue
            
            statement_id = self.create_statement_id(subject, predicate, str(object_val))
            
            statement_doc = {
                "statement_id": statement_id,
                "global_statement_id": f"{video_id}_{statement_id}",
                "statement_uri": stmt["@id"],
                "subject": subject,
                "predicate": predicate,
                "object": str(object_val),
                "source_video": video_id,
                "video_title": video_title,
                "created_at": datetime.now(timezone.utc)
            }
            
            # Extract provenance information
            provenance = stmt.get("prov:wasDerivedFrom")
            if provenance:
                if isinstance(provenance, dict):
                    statement_doc.update({
                        "from_video": provenance.get("bbp:fromVideo", {}).get("@id") if isinstance(provenance.get("bbp:fromVideo"), dict) else provenance.get("bbp:fromVideo"),
                        "start_offset": float(provenance.get("bbp:startTimeOffset", {}).get("@value", 0)) if isinstance(provenance.get("bbp:startTimeOffset"), dict) else provenance.get("bbp:startTimeOffset"),
                        "end_offset": float(provenance.get("bbp:endTimeOffset", {}).get("@value", 0)) if isinstance(provenance.get("bbp:endTimeOffset"), dict) else provenance.get("bbp:endTimeOffset"),
                        "transcript_text": provenance.get("bbp:transcriptText"),
                        "segment_type": provenance.get("@type")
                    })
            
            try:
                self.statements.insert_one(statement_doc)
                statements_added += 1
            except DuplicateKeyError:
                pass  # Skip duplicates
        
        print(f"  âœ… Graph processing complete:")
        print(f"    - Nodes: {nodes_added} added, {nodes_updated} updated")
        print(f"    - Edges: {edges_added} added")
        print(f"    - Statements: {statements_added} added")
        if self.use_embeddings:
            print(f"    - Embeddings: {embeddings_generated} generated")
    
    def save_graph_video_metadata(self, video_data: Dict[str, Any]):
        """Mark video as processed by adding graph_processed flag."""
        try:
            self.videos_source.update_one(
                {"video_id": video_data["video_id"]},
                {
                    "$set": {
                        "graph_processed": True,
                        "graph_processed_at": datetime.now(timezone.utc),
                        "graph_processing_version": "mongodb_graph_loader_v1.0"
                    }
                }
            )
            print(f"    âœ… Marked video as graph processed: {video_data['video_id']}")
            return True
        except Exception as e:
            print(f"    âš ï¸  Failed to mark video as processed: {e}")
            return False
    
    def process_video(self, video_data: Dict[str, Any]) -> bool:
        """Process a single video's JSON-LD data into graph format."""
        video_id = video_data["video_id"]
        video_title = video_data.get("Video_title", "Unknown Title")
        json_ld = video_data.get("json_ld")
        
        if not json_ld:
            print(f"  âš ï¸  No JSON-LD data found for video {video_id}")
            return False
        
        try:
            print(f"  ðŸ“„ JSON-LD data: {len(str(json_ld))} characters")
            
            # Process JSON-LD into graph structure
            self.process_jsonld_to_graph(json_ld, video_id, video_title)
            
            # Mark as processed
            if self.save_graph_video_metadata(video_data):
                return True
            else:
                return False
            
        except Exception as e:
            print(f"  âŒ Error processing video {video_id}: {e}")
            return False
    
    def process_all_videos(self, limit: Optional[int] = None, video_id: Optional[str] = None):
        """
        Process all videos with JSON-LD data into graph format.
        
        Args:
            limit: Maximum number of videos to process
            video_id: Process only a specific video ID
        """
        print("Starting graph loading from JSON-LD data in MongoDB...")
        
        # Get videos with JSON-LD data
        videos_to_process = self.get_videos_with_jsonld(limit=limit, video_id=video_id)
        
        if not videos_to_process:
            print("No videos to process")
            return
        
        stats = {
            "total": len(videos_to_process),
            "processed": 0,
            "errors": 0
        }
        
        for i, video in enumerate(videos_to_process, 1):
            video_id = video["video_id"]
            video_title = video.get("Video_title", "Unknown Title")
            
            print(f"\n[{i}/{stats['total']}] Processing: {video_title[:80]}...")
            print(f"  ðŸ†” Video ID: {video_id}")
            
            # Process the video
            if self.process_video(video):
                stats["processed"] += 1
            else:
                stats["errors"] += 1
        
        # Print final statistics
        print(f"\nðŸ“Š Graph Loading Complete!")
        print(f"  Total videos: {stats['total']}")
        print(f"  Successfully processed: {stats['processed']}")
        print(f"  Errors: {stats['errors']}")
    
    def get_stats(self) -> Dict[str, int]:
        """Get statistics about the loaded graph."""
        stats = {
            "nodes": self.nodes.count_documents({}),
            "edges": self.edges.count_documents({}), 
            "statements": self.statements.count_documents({}),
            "videos_with_jsonld": self.videos_source.count_documents({"json_ld": {"$exists": True}}),
            "videos_graph_processed": self.videos_source.count_documents({"graph_processed": True})
        }
        
        if self.use_embeddings:
            stats["nodes_with_embeddings"] = self.nodes.count_documents({"embedding": {"$exists": True}})
        
        return stats

def main():
    """Main function to run the script."""
    parser = argparse.ArgumentParser(description="Load JSON-LD data from MongoDB into graph collections with vector search support")
    parser.add_argument("--database", default="parliamentary_graph", help="MongoDB database name")
    parser.add_argument("--limit", type=int, help="Limit number of videos to process")
    parser.add_argument("--video-id", help="Process only a specific video ID")
    parser.add_argument("--skip-embeddings", action="store_true", 
                        help="Skip generating vector embeddings (faster processing)")
    parser.add_argument("--stats", action="store_true", help="Show graph statistics only")
    
    args = parser.parse_args()
    
    try:
        # Initialize loader
        loader = MongoDBGraphLoader(
            database_name=args.database,
            use_embeddings=not args.skip_embeddings
        )
        
        if args.stats:
            # Show statistics only
            stats = loader.get_stats()
            print("ðŸ“Š Graph Statistics:")
            for collection, count in stats.items():
                print(f"  {collection}: {count:,} documents")
            return
        
        # Process videos
        loader.process_all_videos(
            limit=args.limit,
            video_id=args.video_id
        )
        
        # Show final statistics
        stats = loader.get_stats()
        print(f"\nðŸ“Š Final Graph Statistics:")
        for collection, count in stats.items():
            print(f"  {collection}: {count:,} documents")
        
    except ValueError as e:
        print(f"Configuration error: {e}")
        print("\nTo set up MongoDB connection:")
        print("1. Create a MongoDB Atlas cluster: https://cloud.mongodb.com/")
        print("2. Get your connection string from the Atlas dashboard")
        print("3. Set environment variable: export MONGODB_CONNECTION_STRING='your-connection-string'")
        print("4. Or create a .env file with: MONGODB_CONNECTION_STRING=your-connection-string")
        sys.exit(1)
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()